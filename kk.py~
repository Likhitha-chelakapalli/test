import pandas as pd
import requests
from bs4 import BeautifulSoup
import time
import random

# Load your CSV file
df = pd.read_csv("sp500-companies.csv5")  # make sure it has 'Company Name' column
df["Official Website"] = ""


# Function to get official website by scraping Google search results
def get_official_website_scrape(company_name):
    try:
        # Prepare search query
        query = f"{company_name} official website"
        url = f"https://www.google.com/search?q={query.replace(' ', '+')}"

        # Set user-agent headers to mimic a browser
        headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
                          "AppleWebKit/537.36 (KHTML, like Gecko) "
                          "Chrome/112.0.0.0 Safari/537.36"
        }

        # Send HTTP GET request
        response = requests.get(url, headers=headers)
        response.raise_for_status()

        # Parse HTML content
        soup = BeautifulSoup(response.text, "html.parser")

        # Google search result links are often inside <a> tags with href
        # Inspect the structure; we look for divs or a tags with class 'yuRUbf' or similar
        for a_tag in soup.select('a'):
            link = a_tag.get('href')
            if link and link.startswith("https://") and company_name.lower().replace(" ", "") in link.lower().replace(
                    " ", ""):
                # Basic heuristic: link contains company name, and is https
                return link
        # Fallback: just return first https link found
        for a_tag in soup.select('a'):
            link = a_tag.get('href')
            if link and link.startswith("https://"):
                return link

    except Exception as e:
        print(f"Error scraping for {company_name}: {e}")
    return ""


# Iterate over dataframe rows and update official websites
for idx, row in df.iterrows():
    company = row["Name"]
    website = get_official_website_scrape(company)
    df.at[idx, "Official Website"] = website
    print(f"{idx + 1}. {company} â†’ {website}")

    # Sleep randomly between 1 to 3 seconds to reduce blocking risk
    time.sleep(random.uniform(1, 3))

# Save updated dataframe to new CSV
df.to_csv("sp500_with_websites_scraped.csv", index=False)
